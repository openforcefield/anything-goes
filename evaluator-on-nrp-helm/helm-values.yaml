---
# nameOverride: dask
# fullnameOverride: dask

scheduler:
  name: scheduler # Dask scheduler name.
  enabled: true # Enable/disable scheduler.
  resources:
    limits:
      cpu: 4
      memory: 8Gi
      ephemeral-storage: 20Gi
    requests:
      cpu: 4
      memory: 8Gi
      ephemeral-storage: 20Gi
  image:
    repository: "ghcr.io/lilyminium/scrapbook-projects"
    tag: "tmp-evaluator-ambertools-distributed-v0"
    # repository: "ghcr.io/dask/dask" # Container image repository.
    # tag: "2024.2.1" # last version of Dask compatible with Pandas 1.5; Evaluator not compatible with Pandas 2 yet
    pullPolicy: IfNotPresent # Container image pull policy.
    pullSecrets: # Container image [pull secrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
    #  - name: regcred
  env:
    - name: EXTRA_CONDA_PACKAGES
      value: "-c conda-forge openff-evaluator=0.4.9 rdkit ambertools pandas=1.5"
    - name: USE_MAMBA
      value: "true"
    - name: DASK_DISTRIBUTED__WORKER__DAEMON
      value: "False"
    # - name: EXTRA_PIP_PACKAGES
    #   value: "pandas=1.5"
  replicas: 1 # should always be 1
  serviceType: "ClusterIP" # Scheduler service type. Set to `LoadBalancer` to expose outside of your cluster. Note you can port-forward to the scheduler service to access it.
  # serviceType: "NodePort"
  # serviceType: "LoadBalancer"
  loadBalancerIP: null # Some cloud providers allow you to specify the loadBalancerIP when using the `LoadBalancer` service type. If your cloud does not support it this option will be ignored.
  servicePort: 8786 # Scheduler service internal port.
  serviceAnnotations: {} # Scheduler service annotations.
  extraArgs:
    [] # Extra CLI arguments to be passed to the scheduler
    # - --preload
    # - scheduler-setup.py
  tolerations: [] # Tolerations.
  affinity: {} # Container affinity.
  nodeSelector: {} # Node Selector.
  securityContext: {} # Security Context.
  # serviceAccountName: ""
  metrics:
    enabled: false # Enable scheduler metrics. Pip package [prometheus-client](https://pypi.org/project/prometheus-client/) should be present on scheduler.
    serviceMonitor:
      enabled: false # Enable scheduler servicemonitor.
      namespace: "" # Deploy servicemonitor in different namespace, e.g. monitoring.
      namespaceSelector: {} # Selector to select which namespaces the Endpoints objects are discovered from.
      # Default: scrape .Release.Namespace only
      # To scrape all, use the following:
      # namespaceSelector:
      #   any: true
      additionalLabels: {} # Additional labels to add to the ServiceMonitor metadata.
      interval: 30s # Interval at which metrics should be scraped.
      jobLabel: "" # The label to use to retrieve the job name from.
      targetLabels: [] # TargetLabels transfers labels on the Kubernetes Service onto the target.
      metricRelabelings: [] # MetricRelabelConfigs to apply to samples before ingestion.
  livenessProbe:
    {} # Enables scheduler liveness probe.
    # httpGet:
    #   path: /health
    #   port: 8787
    # initialDelaySeconds: 3
    # periodSeconds: 10
  readinessProbe:
    {} # Enables scheduler readiness probe.
    # httpGet:
    #   path: /status
    #   port: 8787
    # initialDelaySeconds: 3
    # periodSeconds: 10

webUI:
  name: webui # Dask webui name.
  servicePort: 80 # webui service internal port.
  ingress:
    enabled: false # Enable ingress.
    # ingressClassName:
    pathType: Prefix # set pathType in ingress
    tls: false # Ingress should use TLS.
    # secretName: dask-scheduler-tls
    hostname: dask-ui.example.com # Ingress hostname.
    annotations:
      {} # Ingress annotations. See `values.yaml` for example values.
      # kubernetes.io/ingress.class: "nginx"
      # secretName: my-tls-cert
      # kubernetes.io/tls-acme: "true"

worker:
  name: worker # Dask worker name.
  resources:
    limits:
      cpu: 1
      nvidia.com/gpu: 1
      memory: 6Gi
      ephemeral-storage: 20Gi
    requests:
      cpu: 1
      nvidia.com/gpu: 1
      memory: 6Gi
      ephemeral-storage: 20Gi
  image:
    repository: "ghcr.io/lilyminium/scrapbook-projects"
    tag: "tmp-evaluator-ambertools-distributed-v0"

    # repository: "ghcr.io/dask/dask" # Container image repository.
    # tag: "2024.2.1" # Container image tag.
    pullPolicy: IfNotPresent # Container image pull policy.
    dask_worker: "dask-worker" # Dask worker command. E.g `dask-cuda-worker` for GPU worker.
    # pullSecrets: # Container image [pull secrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
    #  - name: regcred
  replicas: 3 # Number of workers.
  strategy:
    type: RollingUpdate # Strategy used to replace old Pods with new ones.
  custom_scheduler_url: null # connect to already existing scheduler, deployed not by this chart.
  env: # Environment variables. See `values.yaml` for example values.
  #  - name: EXTRA_APT_PACKAGES
  #    value: build-essential openssl
    - name: EXTRA_CONDA_PACKAGES
      value: "openff-evaluator==0.4.9 rdkit ambertools -c conda-forge"
    - name: USE_MAMBA
      value: "true"
    - name: DASK_DISTRIBUTED__WORKER__DAEMON
      value: "False"
    # - name: EXTRA_PIP_PACKAGES
    #   value: "pandas=1.5"
  extraArgs:
    [] # Extra CLI arguments to be passed to the worker
    # - --preload
    # - worker-setup.py
  # resources: {} # Worker pod resources. See `values.yaml` for example values.
  #  limits:
  #    cpu: 1
  #    memory: 3G
  #    nvidia.com/gpu: 1
  #  requests:
  #    cpu: 1
  #    memory: 3G
  #    nvidia.com/gpu: 1
  mounts: # Worker Pod volumes and volume mounts, mounts.volumes follows kuberentes api v1 Volumes spec. mounts.volumeMounts follows kubernetesapi v1 VolumeMount spec
   volumes:
     - name: evaluator-storage
       persistentVolumeClaim:
        claimName: evaluator-storage-lw
   volumeMounts:
     - name: evaluator-storage
       mountPath: /evaluator-storage
       readOnly: false
  annotations: {} # Annotations
  tolerations: [] # Tolerations.
  affinity: {} # Container affinity.
  nodeSelector: {} # Node Selector.
  securityContext: {} # Security Context.
  # serviceAccountName: ""
  # port: ""
  portDashboard: 8790 # Worker dashboard and metrics port.
  #  this option overrides "--nthreads" on workers, which defaults to resources.limits.cpu / default_resources.limits.cpu
  #  use it if you need to limit the amount of threads used by multicore workers, or to make workers with non-whole-number cpu limits
  # threads_per_worker: 1
  metrics:
    enabled: false # Enable workers metrics. Pip package [prometheus-client](https://pypi.org/project/prometheus-client/) should be present on workers.
    podMonitor:
      enabled: false # Enable workers podmonitor
      namespace: "" # Deploy podmonitor in different namespace, e.g. monitoring.
      namespaceSelector: {} # Selector to select which namespaces the Endpoints objects are discovered from.
      # Default: scrape .Release.Namespace only
      # To scrape all, use the following:
      # namespaceSelector:
      #   any: true
      # metrics will apply to the additional worker groups as well
      additionalLabels: {} # Additional labels to add to the PodMonitor metadata.
      interval: 30s # Interval at which metrics should be scraped.
      jobLabel: "" # The label to use to retrieve the job name from.
      podTargetLabels: [] # PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
      metricRelabelings: [] # MetricRelabelConfigs to apply to samples before ingestion.
  livenessProbe:
    {} # Enables worker pod liveness probe
    # httpGet:
    #   path: /health
    #   port: 8790 # The same as portDashboard
    # initialDelaySeconds: 3
    # periodSeconds: 10
  readinessProbe:
    {} # Enables worker pod readiness probe
    # httpGet:
    #   path: /status
    #   port: 8790 # The same as portDashboard
    # initialDelaySeconds: 3
    # periodSeconds: 10

additional_worker_groups: [] # Additional groups of workers to create. List of groups with same options as `worker`.
# - name: high-mem  # Dask worker group name.
#   extraArgs:
#     - --resources
#     - "MEMORY=6e9"
#   resources:
#     limits:
#       memory: 32G
#     requests:
#       memory: 32G
# ...
# (Defaults will be taken from the primary worker configuration)

jupyter:
  name: jupyter # Jupyter name.
  enabled: true # Enable/disable the bundled Jupyter notebook.
  rbac: true # Create RBAC service account and role to allow Jupyter pod to scale worker pods and access logs.
  resources:
    limits:
      cpu: 4
      memory: 8Gi
      ephemeral-storage: 20Gi
    requests:
      cpu: 4
      memory: 8Gi
      ephemeral-storage: 20Gi
  image:
    repository: "ghcr.io/lilyminium/scrapbook-projects"
    tag: "tmp-evaluator-ambertools-distributed-v0"
    # repository: "ghcr.io/dask/dask-notebook" # Container image repository.
    # tag: "2024.2.1" # Container image tag.
    pullPolicy: IfNotPresent # Container image pull policy.
    pullSecrets: # Container image [pull secrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
    #  - name: regcred
    #
  serviceType: "ClusterIP" # Scheduler service type. Set to `LoadBalancer` to expose outside of your cluster.
  # serviceType: "NodePort"
  # serviceType: "LoadBalancer"
  servicePort: 80 # Jupyter service internal port.
  # This hash corresponds to the password 'dask'
  password: "sha1:aae8550c0a44:9507d45e087d5ee481a5ce9f4f16f37a0867318c" # Password hash. Default hash corresponds to the password `dask`.
  passwordHint: "dask" # This is only used in the help text, don't forget to update the hash too.
  env: # Environment variables. See `values.yaml` for example values.
   - name: EXTRA_CONDA_PACKAGES
     value: "openff-evaluator==0.4.9 rdkit ambertools -c conda-forge"
  #  - name: EXTRA_PIP_PACKAGES
  #    value: "pandas=1.5"
  command: null # Container command.
  args: # Container arguments.
  #  - "start.sh"
  #  - "jupyter"
  #  - "lab"
  extraConfig: |-
    # Extra Jupyter config goes here
    # E.g
    # c.NotebookApp.port = 8888

  mounts: # Worker Pod volumes and volume mounts, mounts.volumes follows kuberentes api v1 Volumes spec. mounts.volumeMounts follows kubernetesapi v1 VolumeMount spec
   volumes:
     - name: evaluator-storage
       persistentVolumeClaim:
        claimName: evaluator-storage-lw
   volumeMounts:
     - name: evaluator-storage
       mountPath: /evaluator-storage
       readOnly: false
  tolerations: [] # Tolerations.
  affinity: {} # Container affinity.
  nodeSelector: {} # Node Selector.
  securityContext: {} # Security Context.
  serviceAccountName: "dask-jupyter" # Service account for use with RBAC
  ingress:
    enabled: false # Enable ingress.
    # ingressClassName:
    tls: false # Ingress should use TLS.
    # secretName: dask-jupyter-tls
    pathType: Prefix # set pathType in ingress
    hostname: dask-jupyter.example.com # Ingress hostname.
    annotations:
      {} # Ingress annotations. See `values.yaml` for example values.
      # kuernetesbernetes.io/ingress.class: "nginx"
      # secretName: my-tls-cert
      # kub.io/tls-acme: "true"
  livenessProbe:
    {} # Enables jupyter server liveness probe.
    # httpGet:
    #   path: /login
    #   port: 8888
    # initialDelaySeconds: 3
    # periodSeconds: 10
  readinessProbe:
    {} # Enables jupyter server readiness probe.
    # httpGet:
    #   path: /login
    #   port: 8888
    # initialDelaySeconds: 3
    # periodSeconds: 10